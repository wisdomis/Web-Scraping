{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project ) 웹 스크래핑을 이용하여 나만의 비서 만들기\n",
    "\n",
    "[조건]\n",
    "1. 네이버에서 오늘 서울의 날씨 정보 가져오기\n",
    "2. 헤드라인 뉴스 3건을 가져오기\n",
    "3. IT 뉴스 3건을 가져오기\n",
    "4. 해커스 어학원 홈페이지에서 오늘의 영어 회화 지문을 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[출력 예시]\n",
    "[오늘의 날씨]\n",
    "흐림, 어제보다 00˚ 높아요  \n",
    "현재 온도 00˚ (최저 00˚ / 최고 00˚)  \n",
    "강수확률 오전 00% / 오후 00%  \n",
    "\n",
    "미세먼지 좋음 / 초미세먼지 좋음  \n",
    "\n",
    "[헤드라인 뉴스]\n",
    "1. ~ ~ ~ ~\n",
    "(링크 : http:// ~ ) \n",
    "2. ~ ~ ~ ~\n",
    "(링크 : http:// ~ ) \n",
    "3. ~ ~ ~ ~\n",
    "(링크 : http:// ~ ) \n",
    "\n",
    "[IT 뉴스]\n",
    "1. ~ ~ ~ ~\n",
    "(링크 : http:// ~ ) \n",
    "2. ~ ~ ~ ~\n",
    "(링크 : http:// ~ ) \n",
    "3. ~ ~ ~ ~\n",
    "(링크 : http:// ~ ) \n",
    "\n",
    "[오늘의 영어 회화]\n",
    "(영어 지문)\n",
    "A : bla bla...\n",
    "B : yes, bla bla...\n",
    "\n",
    "(한글 지문)\n",
    "A : 어쩌구 어쩌구...\n",
    "B : 맞아, 저쩌구 저쩌구...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. create_soup\n",
    "\n",
    "def create_soup(url):\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 오늘의 날씨 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seoyu\\anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[오늘의 날씨]\n",
      "어제보다 3° 높아요  흐림 \n",
      "현재 -1° (최저 기온-6° / 최고 기온3°)\n",
      "강수확률  오전 30%  /  오후 30% \n",
      "자외선 좋음 미세먼지 좋음\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_weather():\n",
    "\n",
    "    print(\"[오늘의 날씨]\")\n",
    "    url = \"https://search.naver.com/search.naver?ie=UTF-8&query=%EC%84%9C%EC%9A%B8+%EB%82%A0%EC%94%A8&sm=chr_hty\"\n",
    "    soup = create_soup(url)\n",
    "   \n",
    "    # 어제보다 00˚ 높아요\n",
    "    summary = soup.find(\"p\", attrs = {\"class\":\"summary\"}).get_text()\n",
    "    \n",
    "    \n",
    "    # 현재 00˚ (최저 00˚ / 최고 00˚)\n",
    "    curr_temp = soup.find(\"div\", attrs = {\"class\":\"temperature_text\"}).get_text().strip().replace(\"온도\",\"\")\n",
    "    min_temp = soup.find(\"span\", attrs = {\"class\":\"lowest\"}).get_text().strip().replace(\"최저\",\"\")\n",
    "    max_temp = soup.find(\"span\", attrs = {\"class\":\"highest\"}).get_text().strip().replace(\"최고\",\"\") # 최고 기운\n",
    "    \n",
    "    #오전 강수확률 00% / 오후 강수확률 00%\n",
    "    rain_rate = soup.find_all(\"span\", attrs = {\"class\":\"weather_left\"})\n",
    "    morning = rain_rate[0].get_text()\n",
    "    afternoon = rain_rate[1].get_text()\n",
    "    # 미세먼지\n",
    "    dust = soup.find_all(\"li\", attrs = {\"class\":\"item_today level2\"})\n",
    "    pm10 = dust[0].get_text().strip() # 미세먼지\n",
    "    pm25 = dust[1].get_text().strip() # 초미세먼지\n",
    "    \n",
    "    # 출력\n",
    "    print(summary)\n",
    "    print(\"{} (최저 {} / 최고 {})\".format(curr_temp, min_temp, max_temp))\n",
    "    print(\"강수확률 {} / {}\".format(morning, afternoon))\n",
    "    print(pm10, pm25)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    scrape_weather() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 뉴스 헤드라인 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[헤드라인 뉴스]\n",
      "1 \n",
      "외교부, 日 문화원장 초치..'사도광산 세계유산 등재' 추진 항의\n",
      " (링크 : https://news.v.daum.net/v/20211228172704885)\n",
      "2 \n",
      "50년전 홍합 한그릇 외상값, 美서 200만원 되어 돌아왔다\n",
      " (링크 : https://news.v.daum.net/v/20211228165637899)\n",
      "3 \n",
      "'대만 단교' 니카라과, 대만 대사관 건물도 중국에 넘긴다\n",
      " (링크 : https://news.v.daum.net/v/20211228152701823)\n",
      "4 \n",
      "기업 85%, 폐플라스틱 감축 동참 의향..\"관건은 정책 지원\"\n",
      " (링크 : https://news.v.daum.net/v/20211228144106898)\n"
     ]
    }
   ],
   "source": [
    "def scrape_headline_news():\n",
    "    print(\"[헤드라인 뉴스]\")\n",
    "    url = \"https://news.daum.net/\"\n",
    "    soup = create_soup(url)\n",
    "\n",
    "    headlines = soup.find_all(\"strong\", attrs = {\"class\":\"tit_thumb\"})\n",
    "    for idx,headline in enumerate(headlines):\n",
    "        if idx == 4:\n",
    "            break\n",
    "\n",
    "        title = headline.get_text()\n",
    "        link = headline.a[\"href\"]\n",
    "\n",
    "        print(\"{} {} (링크 : {})\".format(str(idx+1), title, link ))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_headline_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IT 뉴스 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IT 뉴스]\n",
      "1 \n",
      "디지털 전환 확산..'국민 체감 성과' 창출한다\n",
      " (링크 : https://news.v.daum.net/v/20211228162426970)\n",
      "2 \n",
      "\"누리11호 납품 업체 찾습니다\"..우주 개발에 '계약' 방식 도입한다\n",
      " (링크 : https://news.v.daum.net/v/20211228154657621)\n",
      "3 \n",
      "네이버클라우드, 5G 특화망 1호 허가..신사옥 로봇 서비스 활용\n",
      " (링크 : https://news.v.daum.net/v/20211228120502825)\n",
      "4 \n",
      "커피머신·어린이시설 등 생활속 전자파 측정결과 '이상무'\n",
      " (링크 : https://news.v.daum.net/v/20211228120412799)\n"
     ]
    }
   ],
   "source": [
    "def scrape_IT_news():\n",
    "    print(\"[IT 뉴스]\")\n",
    "    url = \"https://news.daum.net/digital#1\"\n",
    "    soup = create_soup(url)\n",
    "\n",
    "    headlines = soup.find_all(\"strong\", attrs = {\"class\":\"tit_thumb\"})\n",
    "    for idx,headline in enumerate(headlines):\n",
    "        if idx == 4:\n",
    "            break\n",
    "        title = headline.get_text()\n",
    "        link = headline.a[\"href\"]\n",
    "        print(\"{} {} (링크 : {})\".format(str(idx+1), title, link))\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_IT_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 영어회화문 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[오늘의 영어회화학습]\n",
      "[영어 지문]\n",
      "I thought your presentation was first class.\n",
      "\n",
      "Mr. Kim: It's an honor to do work that can benefit the company.\n",
      "\n",
      "\n",
      "Mrs. Park: Yes, and it's nice to have such dedicated employees. I thought your presentation was first class.\n",
      "\n",
      "\n",
      "Mr. Kim: Well, I must admit that I received a lot of help from Maria.\n",
      "\n",
      "\n",
      "Mrs. Park: She is very knowledgeable about presentations.\n",
      "\n",
      "\n",
      "[한글 지문]\n",
      "저는 당신의 프레젠테이션이 아주 훌륭했다고 생각해요.\n",
      "\n",
      "Mr. Kim: 회사에 이익이 되는 일을 할 수 있게 된 걸 영광으로 생각하고 있습니다.\n",
      "\n",
      "\n",
      "Mrs. Park: 네, 당신처럼 헌신적인 직원을 만난다는 건 반가운 일입니다. 저는 당신의 프리젠테이션이 아주 훌륭했다고 생각해요.\n",
      "\n",
      "\n",
      "Mr. Kim: 그런데, 저는 이번에 Maria 에게 많은 도움을 받았다는 걸 말씀 드리고 싶어요.\n",
      "\n",
      "\n",
      "Mrs. Park: 그녀는 프레젠테이션에 대해서 정말 아는 것이 많지요.\n"
     ]
    }
   ],
   "source": [
    "def scrape_eng_contents():\n",
    "    print(\"[오늘의 영어회화학습]\")\n",
    "    url = \"https://www.hackers.co.kr/?c=s_eng/eng_contents/I_others_english#;\"\n",
    "    soup = create_soup(url)\n",
    "\n",
    "    dialog = soup.find_all(\"div\", attrs = {\"class\":\"conv_txt\"})\n",
    "\n",
    "    print(\"[영어 지문]\")\n",
    "    print(dialog[1].get_text().strip())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"[한글 지문]\")\n",
    "    print(dialog[0].get_text().strip())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_eng_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
